[{"title": "\nMusic Representing Corpus Virtual: An Open Sourced Library for Explorative Music Generation, Sound Design, and Instrument Creation with Artificial Intelligence and Machine Learning\n      \n    ", "abstract": "\nMusic Representing Corpus Virtual (MRCV) is an open source software suite designed to explore the capabilities of Artificial Intelligence (AI) and Machine Learning (ML) in Music Generation, Sound Design, and Virtual Instrument Creation (MGSDIC). The software is accessible to users of varying levels of experience, with an emphasis on providing an explorative approach to MGSDIC. The main aim of MRCV is to facilitate creativity, allowing users to customize input datasets for training the neural networks, and offering a range of options for each neural network (thoroughly documented in the Github Wiki). The software suite is designed to be accessible to musicians, audio professionals, sound designers, and composers, regardless of their prior experience in AI or ML. The documentation is prepared in such a way as to abstract technical details, thereby making it easy to understand. The software is open source, meaning users can contribute to its development, and the community can collectively benefit from the insights and experience of other users.\n        \u25b3 Less\n", "link": "https://arxiv.org/pdf/2305.14948", "summary": "The Music Representing Corpus Virtual (MRCV) is a software suite that uses Artificial Intelligence (AI) and Machine Learning (ML) to help users create music, sounds, and virtual instruments. The software is designed to be user-friendly for people with different levels of experience, and its main goal is to encourage creativity. MRCV allows users to customize input datasets and offers many options for each of its four neural networks. The first neural network generates audio samples using a mixture of datasets and a user-defined network architecture. The second neural network creates audio plugins based on Gated Recurrent Unit (GRU) networks, which can be used for virtual analog modeling and audio effects. The third neural network generates wavetables for wavetable synthesis using a loss function that calculates the difference between the frequency response of the wavetable and the audio data. The fourth neural network, called Genere, is a code-centric environment for music notation that allows composers to quickly generate and edit graphic scores. \n\nGenere is designed to be a tool for composers to quickly generate graphic scores and to be able to quickly edit them. It is also designed to be able to generate graphic scores that are not possible to be generated by hand, such as scores with thousands of notes or complex shapes. The paper provides examples of scores generated by Genere and the code used to generate them. The software is open source, meaning users can contribute to its development, and the community can benefit from each other\u2019s experience. The paper discusses the future work on the system, such as adding more neural networks, adding more features to Genere, and spreading the word to other people who may be interested in using MRCV. The paper concludes that MRCV is a powerful tool for music generation, sound design, and virtual instrument creation, and it has the potential to revolutionize the field of AI and ML in music."}, {"title": "\n      \n        LoopBoxes -- Evaluation of a Collaborative Accessible Digital Musical Instrument\n      \n    ", "abstract": "\n        LoopBoxes is an accessible digital musical instrument designed to create an intuitive access to loop based music making for children with special educational needs (SEN). This paper describes the evaluation of the instrument in the form of a pilot study during a music festival in Berlin, Germany, as well as a case study with children and music teachers in a SEN school setting. We created a modular system composed of three modules that afford single user as well as collaborative music making. The pilot study was evaluated using informal observation and questionnaires (n = 39), and indicated that the instrument affords music making for people with and without prior musical knowledge across all age groups and fosters collaborative musical processes. The case study was based on observation and a qualitative interview. It confirmed that the instrument meets the needs of the school settings and indicated how future versions could expand access to all students, especially those experiencing complex disabilities. In addition, out-of-the-box functionality seems to be crucial for the long-term implementation of the instrument in a school setting.\n        \u25b3 Less\n", "link": "https://arxiv.org/pdf/2305.14875", "summary": "LoopBoxes is a modular digital musical instrument designed for children and adults with special educational needs (SEN). The instrument is aesthetically appealing and uses tangible interfaces to facilitate accessibility and intuitive interaction. The evaluation of the instrument was conducted through a pilot study during a music festival in Berlin and a case study with children and music teachers in a SEN school setting. The pilot study indicated that the instrument affords music making for people with and without prior musical knowledge across all age groups and fosters collaborative musical processes. The case study confirmed that the instrument meets the needs of the school settings and indicated how future versions could expand access to all students, especially those experiencing complex disabilities. \n\nThe study suggests that didactic materials are necessary to facilitate collaborative music making practices, and that aesthetic communication may be stimulated as soon as the users are sufficiently familiar with the instruments' functions. The modular structure of the instrument was seen as a major advantage and allowed for flexibility in choosing the appropriate modules for individual needs. The desire for a do-it-yourself-kit building set was expressed, and the instrument was enjoyed by children as well as adults ranging from formally trained musicians to hobby musicians and people with no musical experience. \n\nThe study also revealed a list of requirements from the perspective of music teachers in SEN education, including managing fine motor skill requirements of the drum sequencer and the harmonic domination of the instrument. Requests for improvement included the extension of including a way to record 'your own voice' to the instrument and the possibility to choose different sounds. Future iterations will focus on expanding the modules to better serve the population with complex disabilities, and on making the instrument function independently and be ready for use without extended preparation time. \n\nOverall, LoopBoxes is suitable for music making without the need for prior training and is suited to foster a collaborative music making process. The instrument is recommended for children and adults and is suitable for use in SEN school settings."}, {"title": "\n      \n        Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding\n      \n    ", "abstract": "\n        Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).\n  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our \"Collaborative Query Rewriting\" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a \"user Feedback Interaction Graph\" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract the precision degradation from the enlarged index, we introduced additional transformer layers to the L1 retrieval model and added multi-hop affinity and guardrail features to the L2 re-ranking model.\n  Given the production constraints of storage cost and runtime retrieval latency, managing the size of the Collaborative User Index is important. As the user index can be pre-computed, we explored using a Large Language Model (LLM) for multi-hop customer affinity retrieval on the Video/Music domains. In particular, this paper looked into the Dolly-V2 7B model. Given limited user index size, We found the user index derived from fine-tuned Dolly-V2 generation significantly enhanced coverage of unseen user interactions. Consequently, this boosted QR performance on unseen user interactions compared to the graph traversal based user index.\n        \u25b3 Less\n", "link": "https://arxiv.org/pdf/2305.14449", "summary": "The paper proposes a Collaborative Query Rewriting approach to improve conversational AI systems by reducing defects caused by user ambiguities, mistakes, or errors in ASR and NLU. The approach involves building a user Feedback Interaction Graph (FIG) and using multi-hop customer affinity to enrich each user's index. The paper explores using a Large Language Model (LLM) for affinity retrieval in the Video/Music domains. The approach significantly enhances coverage of unseen user interactions and boosts QR performance on unseen user interactions compared to graph traversal-based user index. The paper also introduces guardrail features to prevent false triggers and entity-swap errors. The experiments show that the collaborative approach significantly reduces query defects and improves online performance. The paper concludes with a set of rules for constraint user affinity when building the collaborative user index. The paper discusses the limitations of the approach, such as managing the size of the Collaborative User Index and the computational constraints of graph traversal. The paper investigates the potential of language model generation in enhancing the collaborative approach but notes the computational cost as a primary obstacle to integrating it into production. Overall, the paper presents a promising approach to improving conversational AI systems by leveraging user feedback and multi-hop customer affinity."}]